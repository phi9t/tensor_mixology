# Tensor Mixology

**Tensor Mixology** is an educational PyTorch implementation of a Token Mixer module - an efficient, parameter-free alternative to self-attention mechanisms that achieves linear O(T) complexity instead of quadratic O(T²).

> **⚠️ AI-Generated Code Caveat**: This codebase is primarily generated by AI tools and should be used for educational purposes. While the implementation follows established patterns and best practices, it may require additional testing, optimization, and validation for production use.

## Overview

The Token Mixer performs head-wise feature subspace projections and global token reassembly through tensor reshaping and permutation operations without learnable parameters, maximizing GPU FLOP utilization and memory bandwidth efficiency.

## Key Features

- **Linear Complexity**: O(T) vs O(T²) for self-attention
- **Parameter-Free Mixing**: No learnable weights in the mixing operation
- **Educational Focus**: Comprehensive learning materials and examples
- **Hardware-Conscious Design**: Optimized for GPU memory bandwidth
- **Step-by-Step Documentation**: Visual guides and detailed explanations
- **Interactive Examples**: Learn through hands-on demonstrations

## Installation

This project uses `uv` for Python package management:

```bash
# Clone the repository
git clone <repository-url>
cd tensor-mixology

# Activate virtual environment
source .venv/bin/activate

# Install packages
uv pip install torch torchvision numpy plotly polars rich tqdm
```

## Quick Start

### Basic Token Mixer Usage

```python
from token_mixer import TokenMixer
import torch

# Create a Token Mixer
mixer = TokenMixer(
    num_tokens=32,      # Sequence length
    hidden_dim=768,     # Feature dimension
    num_heads=12        # Number of heads
)

# Process input tensor
x = torch.randn(4, 32, 768)  # (batch, tokens, features)
output = mixer(x)            # Same shape as input

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
```

### Educational Mode

```python
# Enable step-by-step explanations
mixer = TokenMixer(num_tokens=8, hidden_dim=256, num_heads=8)
x = torch.randn(2, 8, 256)

# This will print detailed explanations of each step
output = mixer(x)

# For production use (no prints)
output = mixer.forward_production(x)
```

### Feed Forward Networks

```python
from feed_forward import FeedForward, SwiGLUFeedForward

# Standard FFN
ffn = FeedForward(
    hidden_dim=768,
    ff_dim=3072,       # 4x expansion
    activation="gelu"
)

# Advanced SwiGLU variant
ffn_swiglu = SwiGLUFeedForward(hidden_dim=768, ff_dim=3072)

# Process tokens (with educational output)
output = ffn(x, verbose=True)
```

## Educational Examples

Run interactive examples to understand the architecture:

```bash
# Run all educational examples
python educational_examples.py --all

# Or run interactively
python -i educational_examples.py
run_all_examples()
```

### Available Examples

1. **Tensor Shapes**: Understanding 3D tensors in sequence models
2. **Token Mixer Deep Dive**: Step-by-step tensor operations
3. **Feed Forward Variants**: Comparing different FFN architectures
4. **Architecture Comparison**: Token Mixer vs Self-Attention complexity
5. **Complete Model**: Full transformer-like model walkthrough
6. **Optimization Techniques**: Performance optimization strategies

## Architecture

### Token Mixer Process

The Token Mixer achieves global token interaction through 6 steps:

1. **Head Partitioning**: Split tokens into multiple heads
2. **Dimension Permutation**: Rearrange tensor dimensions
3. **Flatten for Mixing**: Create longer sequence for mixing
4. **Unflatten**: Information redistribution occurs here
5. **Restore Layout**: Reconstruct original tensor shape
6. **Residual + Norm**: Add skip connection and normalize

### Mathematical Formulation

```
Input:  (B, T, D) → Head Split: (B, T, H, d)
        ↓ Permute: (B, H, T, d) → Flatten: (B, H×T, d)
        ↓ Unflatten: (B, H, T, d) → Restore: (B, T, D)
Output: LayerNorm(Input + Mixed)
```

Where:
- `B` = batch size
- `T` = number of tokens
- `D` = hidden dimension
- `H` = number of heads
- `d` = D/H = head dimension

## Performance Analysis

### FLOP Comparison

|----------------|------------|-----------------------------|
| Architecture   | Complexity | FLOPs Formula               |
|----------------|------------|-----------------------------|
| Token Mixer    | O(T)       | ~8 × B × T × D              |
| Self-Attention | O(T²)      | B × T² × D + 4 × B × T × D² |

### Memory Scaling

For sequence length scaling:
- **Token Mixer**: Linear O(T) memory usage
- **Self-Attention**: Quadratic O(T²) attention matrices

Example speedup at T=512, D=768: **~448x fewer FLOPs**

## Documentation

### Core Files

- **README.md** (this file): Overview and quick start
- **MODEL_DOCUMENTATION.md**: Detailed model architecture and implementation
- **VISUAL_GUIDE.md**: Visual tensor shape progressions and diagrams
- **CLAUDE.md**: Development guidance for Claude Code

### Code Structure

```
tensor-mixology/
├── token_mixer.py           # Core TokenMixer implementation
├── feed_forward.py          # Feed forward network variants
├── rank_mixer_model.py      # Complete model architecture
├── educational_examples.py  # Interactive learning examples
└── test_basic.py           # Basic functionality tests
```

## Testing

Run basic tests to verify implementation:

```python
python test_basic.py
```

Tests include:
- Shape invariance verification
- Gradient flow validation
- Error handling
- Zero input behavior
- Parameter counting

## Learning Outcomes

After working with Tensor Mixology, you'll understand:

- ✅ Tensor manipulation techniques in neural networks
- ✅ Linear vs quadratic complexity trade-offs
- ✅ Parameter-efficient architecture design
- ✅ Feed-forward network variants and characteristics
- ✅ Complete transformer-like model construction
- ✅ Performance optimization for production deployment

## Core Dependencies

- **torch** (2.7.1): Core PyTorch framework
- **torchvision** (0.22.1): Computer vision utilities
- **numpy** (2.3.2): Numerical computing
- **plotly** (6.2.0): Data visualization
- **polars** (1.32.0): High-performance data manipulation
- **rich** (14.1.0): Terminal formatting
- **tqdm** (4.67.1): Progress bars

## Contributing

This is an educational project focused on making transformer alternatives accessible. Contributions that improve the learning experience are welcome.

## License

[Add your preferred license here]

## Acknowledgments

This implementation is designed for educational purposes to help engineers and researchers understand efficient alternatives to self-attention mechanisms.
